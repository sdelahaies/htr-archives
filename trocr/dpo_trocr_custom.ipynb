{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = load_dataset(\"sylvain471/trocr-3ch-fr-imc-2-dpo\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.05)\n",
    "train_dataset = dataset[\"train\"] \n",
    "eval_dataset = dataset[\"test\"] \n",
    "\n",
    "# Load model and processor\n",
    "model_name = \"sylvain471/troc-medieval-fr-3ch-imc\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length=32\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "# model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "# model.config.forced_eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = max_length\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to freeze the vision encoder during training\n",
    "for name,param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"encoder.pooler\"):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer, cer\n",
    "\n",
    "\n",
    "# Define collate function\n",
    "def collate_fn(batch):\n",
    "    max_target_length = max_length\n",
    "    images = [\n",
    "        processor(item[\"image\"], return_tensors=\"pt\")[\"pixel_values\"] for item in batch\n",
    "    ]\n",
    "    chosen_texts = [\n",
    "        item[\"chosen\"] + \" \" + tokenizer.eos_token for item in batch\n",
    "    ]  # Ensure EOS token\n",
    "    rejected_texts = [item[\"rejected\"] + \" \" + tokenizer.eos_token for item in batch]\n",
    "\n",
    "    pixel_values = torch.cat(images).to(device)\n",
    "    chosen_inputs = tokenizer(\n",
    "        chosen_texts,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_target_length,\n",
    "    ).to(device)\n",
    "    rejected_inputs = tokenizer(\n",
    "        rejected_texts,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_target_length,\n",
    "    ).to(device)\n",
    "    return pixel_values, chosen_inputs, rejected_inputs\n",
    "\n",
    "\n",
    "# DPO reference free Loss Function\n",
    "def dpo_rf_loss(model, pixel_values, chosen_inputs, rejected_inputs, beta=0.1):\n",
    "    # Compute logits for both the trained and frozen models\n",
    "    chosen_logits = model(\n",
    "        pixel_values=pixel_values, labels=chosen_inputs[\"input_ids\"]\n",
    "    ).logits\n",
    "    rejected_logits = model(\n",
    "        pixel_values=pixel_values, labels=rejected_inputs[\"input_ids\"]\n",
    "    ).logits\n",
    "\n",
    "    chosen_log_probs = F.log_softmax(chosen_logits, dim=-1)\n",
    "    rejected_log_probs = F.log_softmax(rejected_logits, dim=-1)\n",
    "\n",
    "    chosen_token_log_probs = torch.gather(\n",
    "        chosen_log_probs, 2, chosen_inputs[\"input_ids\"].unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    rejected_token_log_probs = torch.gather(\n",
    "        rejected_log_probs, 2, rejected_inputs[\"input_ids\"].unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    chosen_log_prob = chosen_token_log_probs.sum(dim=1)\n",
    "    rejected_log_prob = rejected_token_log_probs.sum(dim=1)\n",
    "\n",
    "    # Compute log-ratio for policy and reference model\n",
    "    policy_ratio = chosen_log_prob - rejected_log_prob\n",
    "    loss = -torch.mean(F.logsigmoid(beta * (policy_ratio)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, processor, dataloader, num_samples=10):\n",
    "    model.eval()\n",
    "    total_cer = 0\n",
    "    total_wer = 0\n",
    "    total_reward_margin = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, (pixel_values, chosen_inputs, rejected_inputs) in enumerate(dataloader):\n",
    "        if count >= num_samples:\n",
    "            break\n",
    "\n",
    "        # Generate model predictions\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        predictions = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Ground truth (chosen texts)\n",
    "        references = processor.tokenizer.batch_decode(chosen_inputs[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "        # Compute CER/WER\n",
    "        batch_cer = sum(cer(ref, pred) for ref, pred in zip(references, predictions)) / len(references)\n",
    "        batch_wer = sum(wer(ref, pred) for ref, pred in zip(references, predictions)) / len(references)\n",
    "\n",
    "        # Compute log probabilities for chosen and rejected responses\n",
    "        chosen_logits = model(pixel_values=pixel_values, labels=chosen_inputs[\"input_ids\"]).logits\n",
    "        rejected_logits = model(pixel_values=pixel_values, labels=rejected_inputs[\"input_ids\"]).logits\n",
    "\n",
    "        chosen_log_probs = torch.log_softmax(chosen_logits, dim=-1)\n",
    "        rejected_log_probs = torch.log_softmax(rejected_logits, dim=-1)\n",
    "\n",
    "        # Sum token log probabilities for each sequence\n",
    "        chosen_token_log_probs = torch.gather(chosen_log_probs, 2, chosen_inputs[\"input_ids\"].unsqueeze(-1)).squeeze(-1)\n",
    "        rejected_token_log_probs = torch.gather(rejected_log_probs, 2, rejected_inputs[\"input_ids\"].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        chosen_log_prob = chosen_token_log_probs.sum(dim=1)  # Summing over sequence length\n",
    "        rejected_log_prob = rejected_token_log_probs.sum(dim=1)\n",
    "\n",
    "        # Compute reward margin\n",
    "        reward_margin = (chosen_log_prob - rejected_log_prob).mean().item()\n",
    "\n",
    "        total_cer += batch_cer\n",
    "        total_wer += batch_wer\n",
    "        total_reward_margin += reward_margin\n",
    "        count += 1\n",
    "\n",
    "    avg_cer = total_cer / count if count > 0 else 0\n",
    "    avg_wer = total_wer / count if count > 0 else 0\n",
    "    avg_reward_margin = total_reward_margin / count if count > 0 else 0\n",
    "    return avg_cer, avg_wer, avg_reward_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test evaluate_model\n",
    "# (pixel_values, chosen_inputs, rejected_inputs)=next(iter(dataloader))\n",
    "# print(len(pixel_values))\n",
    "\n",
    "# generated_ids = model.generate(pixel_values)\n",
    "# predictions = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_accum_steps = 4  # Number of steps before optimizer update\n",
    "eval_every = 15  # Evaluate every N steps\n",
    "eval_samples = 8  # Number of samples for evaluation\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 10\n",
    "step = 0\n",
    "\n",
    "# Set up TensorBoard\n",
    "writer = SummaryWriter(log_dir=\"./runs/trocr_dpo\")\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=eval_samples, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "global_step=0\n",
    "\n",
    "# Log model structure to TensorBoard\n",
    "example_batch = next(iter(train_dataloader))\n",
    "example_pixel_values, example_chosen_inputs, _ = example_batch\n",
    "# writer.add_graph(model, example_pixel_values)  # Log model architecture !! DOES NOT WORK\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    optimizer.zero_grad()  # Start with zeroed gradients\n",
    "\n",
    "    for i, (pixel_values, chosen_inputs, rejected_inputs) in enumerate(progress_bar):\n",
    "        loss = dpo_rf_loss(model, pixel_values, chosen_inputs, rejected_inputs,beta=0.2)  # Reference-free DPO\n",
    "        loss = loss / grad_accum_steps  # Scale loss\n",
    "        loss.backward()  # Backpropagate\n",
    "        \n",
    "        if (i + 1) % grad_accum_steps == 0:  # Update only every `grad_accum_steps` steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            global_step += 1\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            writer.add_scalar(\"Train/Loss\", loss.item() * grad_accum_steps, global_step)\n",
    "            # writer.add_scalar(\"Train/Reward Margin\", reward_margin, global_step)\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=total_loss / (i + 1))\n",
    "        step += 1\n",
    "\n",
    "        # **Evaluation Step**\n",
    "        if step % eval_every == 0:\n",
    "            model.eval()\n",
    "            avg_cer, avg_wer, avg_reward_margin = evaluate_model(model, processor, eval_dataloader, eval_samples)\n",
    "            writer.add_scalar(\"Eval/CER\", avg_cer, global_step)\n",
    "            writer.add_scalar(\"Eval/WER\", avg_wer, global_step)\n",
    "            writer.add_scalar(\"Eval/Reward Margin\", avg_reward_margin, global_step)\n",
    "            print(f\"\\n[Step {step}] CER Loss: {avg_cer:.4f} | Reward margin: {avg_reward_margin:.4f}\")\n",
    "            model.train()  # Back to training\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=17\n",
    "\n",
    "image = eval_dataset[idx]['image'].convert(\"RGB\")\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values.to(\"cuda\"))\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(eval_dataset[idx]['chosen'])\n",
    "print(generated_text)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "token=os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "repo_name=\"<your_hf_repo>\"\n",
    "\n",
    "model.push_to_hub(repo_name,token=token)\n",
    "processor.push_to_hub(repo_name,token=token)\n",
    "tokenizer.push_to_hub(repo_name,token=token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
